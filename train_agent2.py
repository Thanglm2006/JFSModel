import os

# --- Cáº¤U HÃŒNH LÆ¯U CACHE SANG á»” D ---
# Táº¡o thÆ° má»¥c nÃ y trÃªn á»• D trÆ°á»›c náº¿u chÆ°a cÃ³
cache_dir = "D:/huggingface_cache"
if not os.path.exists(cache_dir):
    os.makedirs(cache_dir)

# Thiáº¿t láº­p biáº¿n mÃ´i trÆ°á»ng
os.environ["HF_HOME"] = cache_dir

import pandas as pd
import numpy as np
import torch
import re
import emoji
from datasets import Dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)

# --- Cáº¤U HÃŒNH ---
INPUT_FILE = "Data/step2/content1_step2.csv"
OUTPUT_DIR = "./models/step2_roberta"

#(~560M params)
MODEL_NAME = "xlm-roberta-large"

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)


def normalize_text(text):
    if not isinstance(text, str): return str(text) if text is not None else ""

    # Chuáº©n hÃ³a thá»i gian
    text = re.sub(r'[\/\\]\s*(\d*h|giá»|tiáº¿ng)', ' má»™t giá» ', text, flags=re.IGNORECASE)
    text = re.sub(r'[\/\\]\s*(ngÃ y|day)', ' má»™t ngÃ y ', text, flags=re.IGNORECASE)
    text = re.sub(r'[\/\\]\s*(thÃ¡ng|month)', ' má»™t thÃ¡ng ', text, flags=re.IGNORECASE)

    # Chuáº©n hÃ³a tiá»n (Quan trá»ng cho Scam)
    text = re.sub(r'\b(\d+)\s*(k|ka|xu)\b', r'\1 nghÃ¬n', text, flags=re.IGNORECASE)
    text = re.sub(r'\b(\d+)\s*(tr|triá»‡u|cá»§)\b', r'\1 triá»‡u', text, flags=re.IGNORECASE)

    # Xá»­ lÃ½ emoji
    return emoji.demojize(text, language='alias')


def preprocess_function(examples):
    # Model Large ráº¥t tá»‘n bá»™ nhá»›, truncation=True lÃ  báº¯t buá»™c
    return tokenizer(examples["text"], truncation=True, max_length=512)


def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}


def main():
    print(f"ğŸš€ Báº®T Äáº¦U TRAIN MODEL SCAM (LARGE VERSION)...")
    print(f"âš¡ Model: {MODEL_NAME}")

    # 1. Load Data
    try:
        df = pd.read_csv(INPUT_FILE)
    except FileNotFoundError:
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file {INPUT_FILE}")
        return

    if 'text' not in df.columns and 'content' in df.columns:
        df['text'] = df['content']

    df['text'] = df['text'].apply(normalize_text)
    df = df.dropna(subset=['label'])
    df['label'] = df['label'].astype(int)

    print(f"ğŸ“Š Dá»¯ liá»‡u train: \n{df['label'].value_counts()}")

    # 2. Split Data
    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])
    train_dataset = Dataset.from_pandas(train_df)
    test_dataset = Dataset.from_pandas(test_df)

    tokenized_train = train_dataset.map(preprocess_function, batched=True)
    tokenized_test = test_dataset.map(preprocess_function, batched=True)

    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)

    # 3. Training Config (Tá»I Æ¯U CHO RTX 3080 10GB)
    training_args = TrainingArguments(
        output_dir="./results/scam_large_checkpoints",

        # --- Cáº¤U HÃŒNH VRAM 10GB ---
        per_device_train_batch_size=4,  # Giáº£m batch xuá»‘ng 4 vÃ¬ model Large ráº¥t náº·ng
        gradient_accumulation_steps=8,  # TÃ­ch lÅ©y 8 láº§n -> TÆ°Æ¡ng Ä‘Æ°Æ¡ng batch size 32 (4*8)
        gradient_checkpointing=True,  # ğŸ”¥ QUAN TRá»ŒNG: Giáº£m 50% VRAM, cho phÃ©p train model Large
        fp16=True,  # Báº¯t buá»™c dÃ¹ng FP16 trÃªn 3080 Ä‘á»ƒ nhanh vÃ  nháº¹
        # --------------------------

        learning_rate=1e-5,  # Model Large cáº§n LR nhá» Ä‘á»ƒ á»•n Ä‘á»‹nh
        num_train_epochs=6,  # Model lá»›n há»™i tá»¥ nhanh hÆ¡n, 6-7 epochs lÃ  Ä‘á»§ (trÃ¡nh overfitting)

        weight_decay=0.01,
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        dataloader_num_workers=0,  # Windows báº¯t buá»™c Ä‘á»ƒ 0
        report_to="none"
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_test,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    trainer.train()

    # 4. Save
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    print(f"âœ… ÄÃ£ lÆ°u model Scam Large táº¡i: {OUTPUT_DIR}")


if __name__ == "__main__":
    main()