import os

# --- C·∫§U H√åNH L∆ØU CACHE SANG ·ªî D ---
# T·∫°o th∆∞ m·ª•c n√†y tr√™n ·ªï D tr∆∞·ªõc n·∫øu ch∆∞a c√≥
cache_dir = "D:/huggingface_cache"
if not os.path.exists(cache_dir):
    os.makedirs(cache_dir)

# Thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng
os.environ["HF_HOME"] = cache_dir

import pandas as pd
import numpy as np
import torch
import re
import emoji
from datasets import Dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)

# --- C·∫§U H√åNH ---
INPUT_FILE = "Data/step2/data_train_step2_balanced.csv"
OUTPUT_DIR = "models/step2_mdeberta"

#(~560M params)
MODEL_NAME = "microsoft/mdeberta-v3-base"

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)


def normalize_text(text):
    if not isinstance(text, str): return str(text) if text is not None else ""

    # Chu·∫©n h√≥a th·ªùi gian
    text = re.sub(r'[\/\\]\s*(\d*h|gi·ªù|ti·∫øng)', ' m·ªôt gi·ªù ', text, flags=re.IGNORECASE)
    text = re.sub(r'[\/\\]\s*(ng√†y|day)', ' m·ªôt ng√†y ', text, flags=re.IGNORECASE)
    text = re.sub(r'[\/\\]\s*(th√°ng|month)', ' m·ªôt th√°ng ', text, flags=re.IGNORECASE)

    # Chu·∫©n h√≥a ti·ªÅn (Quan tr·ªçng cho Scam)
    text = re.sub(r'\b(\d+)\s*(k|ka|xu)\b', r'\1 ngh√¨n', text, flags=re.IGNORECASE)
    text = re.sub(r'\b(\d+)\s*(tr|tri·ªáu|c·ªß)\b', r'\1 tri·ªáu', text, flags=re.IGNORECASE)

    # X·ª≠ l√Ω emoji
    return emoji.demojize(text, language='alias')


def preprocess_function(examples):
    # Model Large r·∫•t t·ªën b·ªô nh·ªõ, truncation=True l√† b·∫Øt bu·ªôc
    return tokenizer(examples["text"], truncation=True, max_length=512)


def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}


def main():
    print(f"üöÄ B·∫ÆT ƒê·∫¶U TRAIN MODEL SCAM (LARGE VERSION)...")
    print(f"‚ö° Model: {MODEL_NAME}")

    # 1. Load Data
    try:
        df = pd.read_csv(INPUT_FILE)
    except FileNotFoundError:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file {INPUT_FILE}")
        return

    if 'text' not in df.columns and 'content' in df.columns:
        df['text'] = df['content']

    df['text'] = df['text'].apply(normalize_text)
    df = df.dropna(subset=['label'])
    df['label'] = df['label'].astype(int)

    print(f"üìä D·ªØ li·ªáu train: \n{df['label'].value_counts()}")

    # 2. Split Data
    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])
    train_dataset = Dataset.from_pandas(train_df)
    test_dataset = Dataset.from_pandas(test_df)

    tokenized_train = train_dataset.map(preprocess_function, batched=True)
    tokenized_test = test_dataset.map(preprocess_function, batched=True)

    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)

    # 3. Training Config (T·ªêI ∆ØU CHO RTX 3080 10GB)
    training_args = TrainingArguments(
        output_dir="./results/scam_large_checkpoints",

        learning_rate=2e-5,  # mDeBERTa n√™n ƒë·ªÉ learning rate nh·ªè (1e-5 ho·∫∑c 2e-5)

        # RTX 3080 10GB c√≥ th·ªÉ ch·ªãu ƒë∆∞·ª£c batch 8
        per_device_train_batch_size=8,
        gradient_accumulation_steps=4,
        per_device_eval_batch_size=16,

        num_train_epochs=5,
        weight_decay=0.01,
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        fp16=torch.cuda.is_available(),
        dataloader_num_workers=0,
        report_to="none"
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_test,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    trainer.train()

    # 4. Save
    trainer.save_model(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    print(f"‚úÖ ƒê√£ l∆∞u model Scam Large t·∫°i: {OUTPUT_DIR}")


if __name__ == "__main__":
    main()