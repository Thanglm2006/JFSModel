import re
import torch
import emoji
import torch.nn.functional as F
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from typing import Optional

# --- Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN MODEL ---
# HÃ£y thay Ä‘á»•i Ä‘Æ°á»ng dáº«n nÃ y trá» Ä‘áº¿n Ä‘Ãºng thÆ° má»¥c model báº¡n Ä‘Ã£ train
MODEL_FILTER_PATH = "./models/step1/step1"  # Model 1: Lá»c bÃ i (0: RÃ¡c, 1: Tuyá»ƒn dá»¥ng)
MODEL_SCAM_PATH = "./models/step2/step2"  # Model 2: Check Scam (0: Scam, 1: Legit)

# Khá»Ÿi táº¡o App
app = FastAPI(
    title="JFS - Job Filtering System API",
    description="Há»‡ thá»‘ng lá»c tin tuyá»ƒn dá»¥ng 2 bÆ°á»›c: Lá»c RÃ¡c -> PhÃ¡t hiá»‡n Lá»«a Ä‘áº£o",
    version="2.0"
)

# Thiáº¿t láº­p Device
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"âš¡ Running on: {device}")

# --- BIáº¾N TOÃ€N Cá»¤C Äá»‚ LÆ¯U MODEL ---
models = {}


# --- HÃ€M KHá»žI Táº O (LOAD MODEL KHI START APP) ---
@app.on_event("startup")
async def load_models():
    print("ðŸ”„ Äang táº£i cÃ¡c model lÃªn RAM/VRAM...")
    try:
        # 1. Load Model Filter (BÆ°á»›c 1)
        print(f"   - Loading Model 1 (Filter) from {MODEL_FILTER_PATH}...")
        models['filter_tokenizer'] = AutoTokenizer.from_pretrained(MODEL_FILTER_PATH)
        models['filter_model'] = AutoModelForSequenceClassification.from_pretrained(MODEL_FILTER_PATH)
        models['filter_model'].to(device).eval()

        # 2. Load Model Scam (BÆ°á»›c 2)
        print(f"   - Loading Model 2 (Scam) from {MODEL_SCAM_PATH}...")
        models['scam_tokenizer'] = AutoTokenizer.from_pretrained(MODEL_SCAM_PATH)
        models['scam_model'] = AutoModelForSequenceClassification.from_pretrained(MODEL_SCAM_PATH)
        models['scam_model'].to(device).eval()

        print("âœ… ÄÃ£ táº£i thÃ nh cÃ´ng cáº£ 2 Model!")

    except Exception as e:
        print(f"âŒ Lá»—i nghiÃªm trá»ng khi táº£i model: {e}")
        print("ðŸ’¡ Gá»£i Ã½: Kiá»ƒm tra láº¡i Ä‘Æ°á»ng dáº«n folder model.")
        raise e


# --- HÃ€M Xá»¬ LÃ TEXT (PREPROCESSING) ---
def normalize_text(text):
    if not isinstance(text, str):
        return str(text) if text is not None else ""

    # Chuáº©n hÃ³a thá»i gian
    text = re.sub(r'[\/\\]\s*(\d*h|giá»|tiáº¿ng)', ' má»™t giá» ', text, flags=re.IGNORECASE)
    text = re.sub(r'[\/\\]\s*(ngÃ y|day)', ' má»™t ngÃ y ', text, flags=re.IGNORECASE)
    text = re.sub(r'[\/\\]\s*(thÃ¡ng|month)', ' má»™t thÃ¡ng ', text, flags=re.IGNORECASE)

    # Chuáº©n hÃ³a tiá»n
    text = re.sub(r'\b(\d+)\s*(k|ka|xu)\b', r'\1 nghÃ¬n', text, flags=re.IGNORECASE)
    text = re.sub(r'\b(\d+)\s*(tr|triá»‡u|cá»§)\b', r'\1 triá»‡u', text, flags=re.IGNORECASE)

    # Demojize
    return emoji.demojize(text, language='alias')


# --- DATA MODELS (INPUT/OUTPUT) ---
class TextRequest(BaseModel):
    text: str


class PredictionResponse(BaseModel):
    final_decision: str  # NON_RECRUITMENT | SCAM | LEGIT
    step1_is_recruitment: int
    step1_confidence: float
    step2_is_legit: Optional[int] = None
    step2_confidence: Optional[float] = None
    normalized_text: str


# --- HÃ€M Dá»° ÄOÃN Cá»T LÃ•I ---
def predict_pipeline(text: str):
    clean_text = normalize_text(text)

    # --- BÆ¯á»šC 1: FILTER (RÃ¡c vs Tuyá»ƒn dá»¥ng) ---
    tokenizer1 = models['filter_tokenizer']
    model1 = models['filter_model']

    inputs1 = tokenizer1(clean_text, return_tensors="pt", truncation=True, max_length=512, padding=True).to(device)

    with torch.no_grad():
        outputs1 = model1(**inputs1)
        probs1 = F.softmax(outputs1.logits, dim=-1)

    # Model 1: 0 = RÃ¡c, 1 = Tuyá»ƒn dá»¥ng
    is_recruitment_idx = torch.argmax(probs1, dim=1).item()
    is_recruitment_score = probs1[0][is_recruitment_idx].item()

    # Náº¿u Model 1 báº£o KHÃ”NG PHáº¢I TUYá»‚N Dá»¤NG (0) -> Dá»«ng luÃ´n
    if is_recruitment_idx == 0 and is_recruitment_score>=0.7:
        return {
            "final_decision": "NON_RECRUITMENT",
            "step1_is_recruitment": 0,
            "step1_confidence": round(is_recruitment_score, 4),
            "step2_is_legit": None,
            "step2_confidence": None,
            "normalized_text": clean_text
        }

    # --- BÆ¯á»šC 2: SCAM CHECK (Scam vs Uy tÃ­n) ---
    # Chá»‰ cháº¡y khi BÆ°á»›c 1 lÃ  Tuyá»ƒn dá»¥ng (1)
    tokenizer2 = models['scam_tokenizer']
    model2 = models['scam_model']

    inputs2 = tokenizer2(clean_text, return_tensors="pt", truncation=True, max_length=512, padding=True).to(device)

    with torch.no_grad():
        outputs2 = model2(**inputs2)
        probs2 = F.softmax(outputs2.logits, dim=-1)

    # Model 2: 0 = Scam, 1 = Legit
    is_legit_idx = torch.argmax(probs2, dim=1).item()
    is_legit_score = probs2[0][is_legit_idx].item()

    final_label = "LEGIT" if is_legit_idx == 1 else "SCAM"

    return {
        "final_decision": final_label,
        "step1_is_recruitment": 1,
        "step1_confidence": round(is_recruitment_score, 4),
        "step2_is_legit": is_legit_idx == 1,
        "step2_confidence": round(is_legit_score, 4),
        "normalized_text": clean_text
    }


# --- ENDPOINTS ---
@app.get("/")
def health_check():
    return {"status": "JFS System Ready", "device": device}


@app.post("/predict", response_model=PredictionResponse)
def predict(request: TextRequest):
    if not models:
        raise HTTPException(status_code=503, detail="Models not loaded yet")

    result = predict_pipeline(request.text)
    return result


# # --- CHáº Y SERVER (Náº¾U CHáº Y TRá»°C TIáº¾P) ---
if __name__ == "__main__":
    import uvicorn

    # Cháº¡y server táº¡i localhost:8000
    uvicorn.run(app, host="0.0.0.0", port=8000)