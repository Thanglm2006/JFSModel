import time
import pandas as pd
import random
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager


class JobCrawler:
    def __init__(self):
        options = Options()

        # --- Báº¬T CHáº¾ Äá»˜ CHáº Y NGáº¦M (HEADLESS) Táº I ÄÃ‚Y ---
        options.add_argument("--headless=new")  # Cháº¿ Ä‘á»™ khÃ´ng hiá»‡n cá»­a sá»• (báº£n má»›i nháº¥t)

        # Quan trá»ng: Cáº§n set kÃ­ch thÆ°á»›c giáº£ láº­p, náº¿u khÃ´ng web sáº½ tÆ°á»Ÿng báº¡n Ä‘ang dÃ¹ng Ä‘iá»‡n thoáº¡i vÃ  Ä‘á»•i giao diá»‡n
        options.add_argument("--window-size=1920,1080")

        # CÃ¡c cáº¥u hÃ¬nh chá»‘ng phÃ¡t hiá»‡n bot cÅ©
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_argument(
            "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36")

        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        self.data = []

    def crawl(self, base_url_pattern, job_card_selector, title_selector, link_selector, label, start_page=1,
              max_pages=1):

        for page in range(start_page, start_page + max_pages):
            # --- Tá»° Táº O URL Má»šI ---
            if page == 1:
                # Trang 1 cáº¥u trÃºc thÆ°á»ng khÃ¡c má»™t chÃºt (khÃ´ng cÃ³ chá»¯ trang-1)
                current_url = f"{base_url_pattern}-vi.html"
            else:
                # Trang 2 trá»Ÿ Ä‘i: thÃªm -trang-N
                current_url = f"{base_url_pattern}-trang-{page}-vi.html"

            print(f"ğŸ”„ Äang truy cáº­p Trang {page}: {current_url}")

            try:
                self.driver.get(current_url)
                # Chá»‰ cáº§n chá» load, khÃ´ng cáº§n cuá»™n tÃ¬m nÃºt Next ná»¯a
                time.sleep(3)
            except:
                print(f"âŒ Lá»—i truy cáº­p URL: {current_url}")
                continue

            # --- Tá»ª ÄÃ‚Y TRá» XUá»NG LÃ€ LOGIC CÃ€O JOB NHÆ¯ CÅ¨ ---
            try:
                jobs = WebDriverWait(self.driver, 10).until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, job_card_selector))
                )
            except:
                print("âŒ KhÃ´ng tÃ¬m tháº¥y job nÃ o hoáº·c Ä‘Ã£ háº¿t trang.")
                break

            job_links = []
            for job in jobs:
                try:
                    title = job.find_element(By.CSS_SELECTOR, title_selector).text
                    link = job.find_element(By.CSS_SELECTOR, link_selector).get_attribute('href')
                    if link:
                        job_links.append((title, link))
                except:
                    continue

            print(f"ğŸ” TÃ¬m tháº¥y {len(job_links)} cÃ´ng viá»‡c. Báº¯t Ä‘áº§u láº¥y ná»™i dung...")

            for title, link in job_links:
                try:
                    self.driver.execute_script(f"window.open('{link}', '_blank');")
                    self.driver.switch_to.window(self.driver.window_handles[-1])
                    time.sleep(random.uniform(1, 2))

                    description = "KhÃ´ng láº¥y Ä‘Æ°á»£c ná»™i dung"
                    try:
                        detail_elements = self.driver.find_elements(By.CSS_SELECTOR, ".detail-row")
                        if detail_elements:
                            description = "\n".join([elem.text for elem in detail_elements])
                        else:
                            description = self.driver.find_element(By.TAG_NAME, "body").text
                    except:
                        pass

                    if len(description) > 50:
                        self.data.append({
                            'title': title,
                            'description': description,
                            'label': label,
                            'source': link
                        })

                    self.driver.close()
                    self.driver.switch_to.window(self.driver.window_handles[0])

                except Exception as e:
                    print(f"âš ï¸ Lá»—i job: {e}")
                    if len(self.driver.window_handles) > 1:
                        self.driver.close()
                    self.driver.switch_to.window(self.driver.window_handles[0])

            # --- KHÃ”NG Cáº¦N LOGIC CLICK NÃšT NEXT Ná»®A ---
            print(f"âœ… Xong trang {page}.")
    def save_csv(self, filename="raw_jobs.csv"):
        if not self.data:
            print("âš ï¸ KhÃ´ng cÃ³ dá»¯ liá»‡u nÃ o Ä‘Æ°á»£c cÃ o! Vui lÃ²ng kiá»ƒm tra láº¡i CSS Selector hoáº·c Ä‘Æ°á»ng truyá»n.")
            return
        df = pd.DataFrame(self.data)
        # LÃ m sáº¡ch cÆ¡ báº£n: XÃ³a xuá»‘ng dÃ²ng thá»«a
        df['description'] = df['description'].apply(lambda x: x.replace('\n', ' ').strip())
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… ÄÃ£ lÆ°u {len(df)} dÃ²ng vÃ o file {filename}")
        self.driver.quit()


# --- HÆ¯á»šNG DáºªN Sá»¬ Dá»¤NG ---

crawler = JobCrawler()

# VÃ Dá»¤ 1: CÃ o CareerViet (Nguá»“n uy tÃ­n - Label 1)
# URL: https://careerviet.vn/viec-lam/tat-ca-viec-lam-vi.html
# CÃ¡ch láº¥y CSS Selector: Chuá»™t pháº£i vÃ o TiÃªu Ä‘á» job -> Inspect (Kiá»ƒm tra) -> Xem class
print("ğŸ•·ï¸ Báº¯t Ä‘áº§u cÃ o CareerViet...")
crawler.crawl(
    # LÆ°u Ã½: Cáº¯t bá» "-vi.html" á»Ÿ cuá»‘i, chá»‰ Ä‘á»ƒ láº¡i pháº§n gá»‘c
    base_url_pattern="https://careerviet.vn/viec-lam/tat-ca-viec-lam",

    job_card_selector=".job-item",
    title_selector=".title a",
    link_selector=".title a",
    label=1,
    start_page=1,  # Báº¯t Ä‘áº§u tá»« trang 1
    max_pages=50  # CÃ o 5 trang (Trang 1 -> Trang 5)
)
# # VÃ Dá»¤ 2: CÃ o Muaban.net (Nguá»“n há»—n há»£p/tiá»m nÄƒng lá»«a Ä‘áº£o - Label 0)
# # LÆ°u Ã½: Muaban.net cÃ³ cáº£ viá»‡c tháº­t, báº¡n cÃ o xong pháº£i lá»c tay láº¡i nhá»¯ng bÃ i lá»«a Ä‘áº£o Ä‘á»ƒ gÃ¡n Label 0
# print("ğŸ•·ï¸ Báº¯t Ä‘áº§u cÃ o Muaban.net...")
# crawler.crawl(
#     url="https://muaban.net/viec-lam-tuyen-dung-toan-quoc-l0-c100",
#     job_card_selector=".list-item-container",  # Cáº§n F12 Ä‘á»ƒ check láº¡i class nÃ y tÃ¹y thá»i Ä‘iá»ƒm
#     title_selector=".title",
#     link_selector=".title a",
#     label=0  # Táº¡m gÃ¡n lÃ  0, sau nÃ y lá»c láº¡i
# )

crawler.save_csv("data_viet.csv")