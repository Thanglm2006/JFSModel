import re

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch.nn.functional as F
import emoji
model_path = "./my_scam_model"
device = "cuda" if torch.cuda.is_available() else "cpu"

print(f"Running on: {device}")

loaded_tokenizer = AutoTokenizer.from_pretrained(model_path)
loaded_model = AutoModelForSequenceClassification.from_pretrained(model_path)
loaded_model.to(device)
loaded_model.eval()

print("Model loaded")


def predict_scam(text):
    # Tokenize input
    inputs = loaded_tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=512,
        padding=True
    ).to(device)

    with torch.no_grad():
        outputs = loaded_model(**inputs)

    logits = outputs.logits

    # use softmax to turn res to probabilities
    probs = F.softmax(logits, dim=-1)

    # take the highest probability
    pred_label_idx = torch.argmax(probs, dim=1).item()
    confidence = probs[0][pred_label_idx].item()

    label_map = {0: "‚ö†Ô∏è L·ª™A ƒê·∫¢O (SCAM)", 1: "‚úÖ UY T√çN (LEGIT)"}

    return label_map[pred_label_idx], confidence, probs[0].tolist()

def convert_emoji(text):
    if not isinstance(text, str): # N·∫øu kh√¥ng ph·∫£i chu·ªói (v√≠ d·ª• l√† nan/float)
        return str(text)
    return emoji.demojize(text, language='alias')


def normalize_text(text):
    # 1. Ki·ªÉm tra an to√†n tr∆∞·ªõc ti√™n
    if not isinstance(text, str):
        return str(text) if text is not None else ""

    # L∆ØU √ù: ƒê√£ x√≥a d√≤ng text = text.lower()

    # 2. Chu·∫©n h√≥a th·ªùi gian/l∆∞∆°ng (Th√™m flags=re.IGNORECASE)
    # B·∫Øt: /h, /H, /gi·ªù, /Gi·ªù...
    text = re.sub(r'[\/\\]\s*(\d*h|gi·ªù|ti·∫øng)', ' m·ªôt gi·ªù ', text, flags=re.IGNORECASE)

    # B·∫Øt: /ng√†y, /Day, /Ng√†y...
    text = re.sub(r'[\/\\]\s*(ng√†y|day)', ' m·ªôt ng√†y ', text, flags=re.IGNORECASE)

    # B·∫Øt: /th√°ng, /Month...
    text = re.sub(r'[\/\\]\s*(th√°ng|month)', ' m·ªôt th√°ng ', text, flags=re.IGNORECASE)

    # 3. Chu·∫©n h√≥a ƒë∆°n v·ªã ti·ªÅn
    # B·∫Øt: 100k, 100K, 100ka, 100KA
    text = re.sub(r'\b(\d+)\s*(k|ka)\b', r'\1 ngh√¨n', text, flags=re.IGNORECASE)

    # B·∫Øt: 5tr, 5TR, 5Tr, 5c·ªß...
    text = re.sub(r'\b(\d+)\s*(tr|tri·ªáu|c·ªß)\b', r'\1 tri·ªáu', text, flags=re.IGNORECASE)

    # 4. Demojize (Chuy·ªÉn icon th√†nh text :smile:)
    return emoji.demojize(text, language='alias')
test_texts = [
    "Tuy·ªÉn d·ª•ng nh√¢n vi√™n nh·∫≠p li·ªáu t·∫°i nh√†, kh√¥ng c·∫ßn c·ªçc, l∆∞∆°ng 500k/ng√†y, inbox nh·∫≠n vi·ªác ngay üí∞üí∞üí∞",
    "C√¥ng ty FPT Software tuy·ªÉn d·ª•ng K·ªπ s∆∞ c·∫ßu n·ªëi (BrSE), y√™u c·∫ßu ti·∫øng Nh·∫≠t N2, kinh nghi·ªám 2 nƒÉm.",
        "üî•Qu√°n cafe √¥ng k·∫π tuy·ªÉn nh√¢n vi√™n ph·ª•c v·ª•, l∆∞∆°ng 20k/h, ‚úÖl·ªãch l√†m: 7h-11h t·ª´ th·ª© 2 ƒë·∫øn th·ª© 7."
]

print("\n--- K·∫æT QU·∫¢ D·ª∞ ƒêO√ÅN ---")
for t in test_texts:
    label, conf, all_probs = predict_scam(normalize_text(t))
    print(f"üìù Text: {normalize_text(t)}")
    print(f"üéØ Result: {label}")
    print(f"üìä Accuracy: {conf:.2%}")
    print(f"üìâ Probabilit√≠e: [L·ª´a ƒë·∫£o: {all_probs[0]:.2%}, Uy t√≠n: {all_probs[1]:.2%}]")
    print("-" * 30)
