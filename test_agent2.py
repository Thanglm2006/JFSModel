import re

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch.nn.functional as F
import emoji
model_path = "models/step2_mdeberta"
device = "cuda" if torch.cuda.is_available() else "cpu"

print(f"Running on: {device}")

loaded_tokenizer = AutoTokenizer.from_pretrained(model_path)
loaded_model = AutoModelForSequenceClassification.from_pretrained(model_path)
loaded_model.to(device)
loaded_model.eval()

print("Model loaded")


def predict_scam(text):
    # Tokenize input
    inputs = loaded_tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=512,
        padding=True
    ).to(device)

    with torch.no_grad():
        outputs = loaded_model(**inputs)

    logits = outputs.logits

    # use softmax to turn res to probabilities
    probs = F.softmax(logits, dim=-1)

    # take the highest probability
    pred_label_idx = torch.argmax(probs, dim=1).item()
    confidence = probs[0][pred_label_idx].item()

    label_map = {0: "âš ï¸ Lá»ªA Äáº¢O (SCAM)", 1: "âœ… UY TÃN (LEGIT)"}

    return label_map[pred_label_idx], confidence, probs[0].tolist()

def convert_emoji(text):
    if not isinstance(text, str): # Náº¿u khÃ´ng pháº£i chuá»—i (vÃ­ dá»¥ lÃ  nan/float)
        return str(text)
    return emoji.demojize(text, language='alias')


def normalize_text(text):
    # 1. Kiá»ƒm tra an toÃ n trÆ°á»›c tiÃªn
    if not isinstance(text, str):
        return str(text) if text is not None else ""

    # LÆ¯U Ã: ÄÃ£ xÃ³a dÃ²ng text = text.lower()

    # 2. Chuáº©n hÃ³a thá»i gian/lÆ°Æ¡ng (ThÃªm flags=re.IGNORECASE)
    # Báº¯t: /h, /H, /giá», /Giá»...
    text = re.sub(r'[\/\\]\s*(\d*h|giá»|tiáº¿ng)', ' má»™t giá» ', text, flags=re.IGNORECASE)

    # Báº¯t: /ngÃ y, /Day, /NgÃ y...
    text = re.sub(r'[\/\\]\s*(ngÃ y|day)', ' má»™t ngÃ y ', text, flags=re.IGNORECASE)

    # Báº¯t: /thÃ¡ng, /Month...
    text = re.sub(r'[\/\\]\s*(thÃ¡ng|month)', ' má»™t thÃ¡ng ', text, flags=re.IGNORECASE)

    # 3. Chuáº©n hÃ³a Ä‘Æ¡n vá»‹ tiá»n
    # Báº¯t: 100k, 100K, 100ka, 100KA
    text = re.sub(r'\b(\d+)\s*(k|ka)\b', r'\1 nghÃ¬n', text, flags=re.IGNORECASE)

    # Báº¯t: 5tr, 5TR, 5Tr, 5cá»§...
    text = re.sub(r'\b(\d+)\s*(tr|triá»‡u|cá»§|m)\b', r'\1 triá»‡u', text, flags=re.IGNORECASE)

    # 4. Demojize (Chuyá»ƒn icon thÃ nh text :smile:)
    return emoji.demojize(text, language='alias')
test_texts = [
    "Tuyá»ƒn dá»¥ng nhÃ¢n viÃªn nháº­p liá»‡u táº¡i nhÃ , khÃ´ng cáº§n cá»c, lÆ°Æ¡ng 500k/ngÃ y, inbox nháº­n viá»‡c ngay ğŸ’°ğŸ’°ğŸ’°",
    "CÃ´ng ty FPT Software tuyá»ƒn dá»¥ng Ká»¹ sÆ° cáº§u ná»‘i (BrSE), yÃªu cáº§u tiáº¿ng Nháº­t N2, kinh nghiá»‡m 2 nÄƒm.",
        "ğŸ”¥QuÃ¡n cafe Ã´ng káº¹ tuyá»ƒn nhÃ¢n viÃªn phá»¥c vá»¥, lÆ°Æ¡ng 20k/h, âœ…lá»‹ch lÃ m: 7h-11h tá»« thá»© 2 Ä‘áº¿n thá»© 7.",
    """
    ğŸ¥‡Chá»— mÃ¬nh Ä‘ang cáº§n ngÆ°á»i phá»¥ bÃ¡n hÃ ng cÃ¡c máº·t hÃ ng organic.
ğŸ¥‡Thá»i gian tá»« 8h Ä‘áº¿n 17h chiá»u
ğŸ¥‡CÃ³ thá»ƒ lÃ m cáº£ ngÃ y hay 1 buá»•i.
 - Thá»i gian: 
+ SÃ¡ng tá»« 8h - 11h30
+ Chiá»u tá»« 13h30 - 17h00
ğŸ¥‡Tuá»•i 
+Nam tá»« 20 tuá»•i - 30 tuá»•i
+Ná»¯ tá»« 20 tuá»•i - 55 tuá»•i
Báº¡n nÃ o cÃ³ nhu cáº§u xin liÃªn há»‡ 0932580161
"""
]

print("\n--- Káº¾T QUáº¢ Dá»° ÄOÃN ---")
for t in test_texts:
    label, conf, all_probs = predict_scam(normalize_text(t))
    print(f"ğŸ“ Text: {normalize_text(t)}")
    print(f"ğŸ¯ Result: {label}")
    print(f"ğŸ“Š Accuracy: {conf:.2%}")
    print(f"ğŸ“‰ ProbabilitÃ­e: [Lá»«a Ä‘áº£o: {all_probs[0]:.2%}, Uy tÃ­n: {all_probs[1]:.2%}]")
    print("-" * 30)
