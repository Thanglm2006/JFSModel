import pandas as pd
import torch
import numpy as np
import os
from sklearn.feature_extraction.text import TfidfVectorizer

# --- C·∫§U H√åNH ---
INPUT_FILES = ["facebook","data_viet"]
TARGET_COLUMN = 'text'
THRESHOLD = 0.90  # 90%
BATCH_SIZE = 2000  # Gi·∫£m nh·∫π batch size ƒë·ªÉ an to√†n h∆°n

# Ki·ªÉm tra GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"üöÄ ƒêang ch·∫°y tr√™n thi·∫øt b·ªã: {device}")


def process_gpu_duplicates():
    # 1. ƒê·ªåC D·ªÆ LI·ªÜU
    all_data = []
    print("--- Giai ƒëo·∫°n 1: ƒê·ªçc d·ªØ li·ªáu ---")
    for filename in INPUT_FILES:
        file_path = f"raw/{filename}.csv"
        if os.path.exists(file_path):
            try:
                df = pd.read_csv(file_path)
                df['__source_file__'] = filename
                df['__original_index__'] = df.index
                df[TARGET_COLUMN] = df[TARGET_COLUMN].fillna("").astype(str)
                all_data.append(df)
                print(f"‚úÖ ƒê√£ t·∫£i: {filename}.csv ({len(df)} d√≤ng)")
            except Exception as e:
                print(f"‚ùå L·ªói ƒë·ªçc file {filename}: {e}")

    if not all_data:
        print("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·∫ßu v√†o.")
        return

    combined_df = pd.concat(all_data, ignore_index=True)
    texts = combined_df[TARGET_COLUMN].tolist()

    print(f"\n--- Giai ƒëo·∫°n 2: Vector h√≥a d·ªØ li·ªáu ({len(texts)} d√≤ng) ---")

    # Tinh ch·ªânh vectorizer ƒë·ªÉ x·ª≠ l√Ω ti·∫øng Vi·ªát v√† so kh·ªõp m·ªù t·ªët h∆°n
    vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 4), min_df=1)
    tfidf_matrix = vectorizer.fit_transform(texts)

    print("--- Giai ƒëo·∫°n 3: T√≠nh to√°n t∆∞∆°ng ƒë·ªìng tr√™n GPU ---")

    # --- PH·∫¶N S·ª¨A L·ªñI ---
    # Chuy·ªÉn ƒë·ªïi ƒë·ªãnh d·∫°ng Sparse Matrix t·ª´ Scikit-learn (COO) sang PyTorch
    coo = tfidf_matrix.tocoo()

    # T·∫°o indices (d·∫°ng [2, N]) v√† values
    indices = np.vstack((coo.row, coo.col))

    # Chuy·ªÉn sang Tensor
    i = torch.LongTensor(indices)
    v = torch.FloatTensor(coo.data)
    shape = coo.shape

    # D√πng sparse_coo_tensor (H√†m m·ªõi thay th·∫ø sparse_FloatTensor)
    sparse_tensor = torch.sparse_coo_tensor(i, v, torch.Size(shape)).to(device)
    # --------------------

    # Chuy·ªÉn sang Dense matrix ƒë·ªÉ t√≠nh to√°n nhanh (V√¨ d·ªØ li·ªáu < 10.000 d√≤ng n√™n RAM GPU ch·ªãu t·ªët)
    # N·∫øu b·ªã l·ªói Out Of Memory ·ªü d√≤ng n√†y, h√£y b√°o m√¨nh ƒë·ªÉ ƒë·ªïi sang c√°ch t√≠nh t·ª´ng l√¥ (batch)
    try:
        full_dense = sparse_tensor.to_dense()
    except RuntimeError as e:
        print(f"‚ùå L·ªói b·ªô nh·ªõ GPU: {e}")
        print("üí° Gi·∫£i ph√°p: Gi·∫£m d·ªØ li·ªáu ho·∫∑c chuy·ªÉn sang CPU.")
        return

    drop_indices = set()
    n_samples = full_dense.shape[0]

    print(f"ƒêang qu√©t tr√πng l·∫∑p tr√™n {n_samples} d√≤ng...")

    # V√≤ng l·∫∑p x·ª≠ l√Ω theo Batch
    for i in range(0, n_samples, BATCH_SIZE):
        end = min(i + BATCH_SIZE, n_samples)

        # L·∫•y batch hi·ªán t·∫°i
        batch_vectors = full_dense[i:end]

        # T√≠nh ma tr·∫≠n t∆∞∆°ng ƒë·ªìng: Batch x All
        sim_matrix = torch.mm(batch_vectors, full_dense.T)

        # Chuy·ªÉn k·∫øt qu·∫£ v·ªÅ CPU ƒë·ªÉ x·ª≠ l√Ω logic (tr√°nh thao t√°c index ph·ª©c t·∫°p tr√™n GPU)
        sim_vals_cpu = sim_matrix.cpu().numpy()

        for local_idx in range(end - i):
            global_idx = i + local_idx

            if global_idx in drop_indices:
                continue

            # L·∫•y d√≤ng t∆∞∆°ng ƒë·ªìng
            row_sims = sim_vals_cpu[local_idx]

            # T√¨m c√°c d√≤ng TR∆Ø·ªöC d√≤ng hi·ªán t·∫°i c√≥ ƒë·ªô gi·ªëng > THRESHOLD
            # Ch√∫ng ta ch·ªâ quan t√¢m [:global_idx] v√¨ mu·ªën gi·ªØ d√≤ng xu·∫•t hi·ªán tr∆∞·ªõc, x√≥a d√≤ng sau
            previous_matches = np.where(row_sims[:global_idx] >= THRESHOLD)[0]

            # Ki·ªÉm tra xem b·∫£n g·ªëc c·ªßa n√≥ c√≥ b·ªã x√≥a ch∆∞a?
            # N·∫øu b·∫£n g·ªëc (d√≤ng xu·∫•t hi·ªán tr∆∞·ªõc) v·∫´n c√≤n -> D√≤ng n√†y l√† th·ª´a -> X√≥a
            is_duplicate = False
            for match_idx in previous_matches:
                if match_idx not in drop_indices:
                    is_duplicate = True
                    break

            if is_duplicate:
                drop_indices.add(global_idx)

        print(f"‚úÖ ƒê√£ x·ª≠ l√Ω xong batch {i}-{end}")

    # 4. L·ªåC V√Ä XU·∫§T FILE
    print(f"\n--- Giai ƒëo·∫°n 4: Xu·∫•t k·∫øt qu·∫£ ---")

    all_indices = set(range(len(combined_df)))
    keep_indices = list(all_indices - drop_indices)
    keep_indices.sort()

    df_clean_global = combined_df.iloc[keep_indices]

    print(f"T·ªïng ban ƒë·∫ßu: {len(combined_df)} | Sau khi l·ªçc: {len(df_clean_global)}")
    print(f"ƒê√£ lo·∫°i b·ªè: {len(drop_indices)} d√≤ng.")

    for filename in INPUT_FILES:
        df_part = df_clean_global[df_clean_global['__source_file__'] == filename].copy()

        # X√≥a c√°c c·ªôt t·∫°m
        if '__source_file__' in df_part.columns:
            del df_part['__source_file__']
        if '__original_index__' in df_part.columns:
            del df_part['__original_index__']

        output_name = f"{filename}_unique.csv"
        df_part.to_csv(output_name, index=False, encoding='utf-8-sig')
        print(f"üìÅ ƒê√£ l∆∞u: {output_name} ({len(df_part)} d√≤ng)")


# --- CH·∫†Y ---
if __name__ == "__main__":
    if torch.cuda.is_available():
        process_gpu_duplicates()
    else:
        print("‚ùå Kh√¥ng t√¨m th·∫•y GPU NVIDIA. Vui l√≤ng c√†i ƒë·∫∑t CUDA.")