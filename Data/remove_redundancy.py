import pandas as pd
import torch
import numpy as np
import os
from sklearn.feature_extraction.text import TfidfVectorizer

# --- C·∫§U H√åNH ---
INPUT_FILES = ["raw/facebook", "raw/data_viet"]  # T√™n file c·ªßa b·∫°n
TARGET_COLUMN = 'text'
THRESHOLD = 0.90  # Ng∆∞·ª°ng tr√πng l·∫∑p 90%
BATCH_SIZE = 2000  # Gi·∫£m n·∫øu b·ªã l·ªói Memory

# Ki·ªÉm tra GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"üöÄ ƒêang ch·∫°y tr√™n thi·∫øt b·ªã: {device}")


def process_gpu_duplicates():
    # 1. ƒê·ªåC D·ªÆ LI·ªÜU V√Ä G·ªòP C·ªòT
    all_data = []
    print("--- Giai ƒëo·∫°n 1: ƒê·ªçc v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu ---")

    for filename in INPUT_FILES:
        file_path = f"{filename}.csv"
        if os.path.exists(file_path):
            try:
                df = pd.read_csv(file_path)

                # --- LOGIC G·ªòP 2 C·ªòT ƒê·∫¶U TI√äN ---
                # Ki·ªÉm tra xem file c√≥ √≠t nh·∫•t 2 c·ªôt kh√¥ng
                if len(df.columns) >= 2:
                    # L·∫•y n·ªôi dung 2 c·ªôt ƒë·∫ßu (index 0 v√† 1)
                    # .fillna('') ƒë·ªÉ x·ª≠ l√Ω √¥ tr·ªëng th√†nh chu·ªói r·ªóng
                    # .astype(str) ƒë·ªÉ ƒë·∫£m b·∫£o l√† chu·ªói
                    col_1 = df.iloc[:, 0].fillna('').astype(str)
                    col_2 = df.iloc[:, 1].fillna('').astype(str)

                    # G·ªôp l·∫°i th√†nh c·ªôt 'text', ngƒÉn c√°ch b·ªüi xu·ªëng d√≤ng
                    df[TARGET_COLUMN] = col_1 + "\n" + col_2
                    print(f"   ‚ÑπÔ∏è ƒê√£ g·ªôp c·ªôt '{df.columns[0]}' v√† '{df.columns[1]}' th√†nh 'text'.")
                else:
                    print(f"   ‚ö†Ô∏è File {filename} c√≥ √≠t h∆°n 2 c·ªôt, b·ªè qua b∆∞·ªõc g·ªôp.")
                    # N·∫øu kh√¥ng g·ªôp ƒë∆∞·ª£c th√¨ ph·∫£i ƒë·∫£m b·∫£o c√≥ c·ªôt text, n·∫øu kh√¥ng t·∫°o r·ªóng
                    if TARGET_COLUMN not in df.columns:
                        df[TARGET_COLUMN] = ""
                # ------------------------------------

                df['__source_file__'] = filename
                df['__original_index__'] = df.index

                # L√†m s·∫°ch d·ªØ li·ªáu l·∫ßn cu·ªëi ƒë·ªÉ tr√°nh l·ªói
                df[TARGET_COLUMN] = df[TARGET_COLUMN].fillna("").astype(str)

                all_data.append(df)
                print(f"‚úÖ ƒê√£ t·∫£i: {filename}.csv ({len(df)} d√≤ng)")
            except Exception as e:
                print(f"‚ùå L·ªói ƒë·ªçc file {filename}: {e}")

    if not all_data:
        print("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·∫ßu v√†o.")
        return

    combined_df = pd.concat(all_data, ignore_index=True)
    texts = combined_df[TARGET_COLUMN].tolist()

    print(f"\n--- Giai ƒëo·∫°n 2: Vector h√≥a d·ªØ li·ªáu ({len(texts)} d√≤ng) ---")

    # Vector h√≥a
    vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 4), min_df=1)
    tfidf_matrix = vectorizer.fit_transform(texts)

    print("--- Giai ƒëo·∫°n 3: T√≠nh to√°n t∆∞∆°ng ƒë·ªìng tr√™n GPU ---")

    # Chuy·ªÉn ƒë·ªïi sang PyTorch Sparse Tensor
    coo = tfidf_matrix.tocoo()
    indices = np.vstack((coo.row, coo.col))
    i = torch.LongTensor(indices)
    v = torch.FloatTensor(coo.data)
    shape = coo.shape

    sparse_tensor = torch.sparse_coo_tensor(i, v, torch.Size(shape)).to(device)

    try:
        full_dense = sparse_tensor.to_dense()
    except RuntimeError as e:
        print(f"‚ùå L·ªói b·ªô nh·ªõ GPU: {e}")
        return

    drop_indices = set()
    n_samples = full_dense.shape[0]

    print(f"ƒêang qu√©t tr√πng l·∫∑p tr√™n {n_samples} d√≤ng...")

    for i in range(0, n_samples, BATCH_SIZE):
        end = min(i + BATCH_SIZE, n_samples)
        batch_vectors = full_dense[i:end]
        sim_matrix = torch.mm(batch_vectors, full_dense.T)
        sim_vals_cpu = sim_matrix.cpu().numpy()

        for local_idx in range(end - i):
            global_idx = i + local_idx
            if global_idx in drop_indices: continue

            row_sims = sim_vals_cpu[local_idx]
            previous_matches = np.where(row_sims[:global_idx] >= THRESHOLD)[0]

            is_duplicate = False
            for match_idx in previous_matches:
                if match_idx not in drop_indices:
                    is_duplicate = True
                    break

            if is_duplicate:
                drop_indices.add(global_idx)

        print(f"‚úÖ ƒê√£ x·ª≠ l√Ω xong batch {i}-{end}", end='\r')

    # 4. XU·∫§T FILE
    print(f"\n\n--- Giai ƒëo·∫°n 4: Xu·∫•t k·∫øt qu·∫£ ---")

    all_indices = set(range(len(combined_df)))
    keep_indices = list(all_indices - drop_indices)
    keep_indices.sort()

    df_clean_global = combined_df.iloc[keep_indices]

    print(f"T·ªïng ban ƒë·∫ßu: {len(combined_df)} | Sau khi l·ªçc: {len(df_clean_global)}")
    print(f"ƒê√£ lo·∫°i b·ªè: {len(drop_indices)} d√≤ng.")

    for filename in INPUT_FILES:
        df_part = df_clean_global[df_clean_global['__source_file__'] == filename].copy()

        # X√≥a c·ªôt t·∫°m
        cols_to_drop = ['__source_file__', '__original_index__']
        # N·∫øu b·∫°n kh√¥ng mu·ªën gi·ªØ l·∫°i c·ªôt 'text' g·ªôp trong file k·∫øt qu·∫£, b·ªè comment d√≤ng d∆∞·ªõi:
        # cols_to_drop.append('text')

        df_part = df_part.drop(columns=[c for c in cols_to_drop if c in df_part.columns])

        output_name = f"{filename}_filtered.csv"
        df_part.to_csv(output_name, index=False, encoding='utf-8-sig')
        print(f"üìÅ ƒê√£ l∆∞u: {output_name} ({len(df_part)} d√≤ng)")


if __name__ == "__main__":
    if torch.cuda.is_available():
        process_gpu_duplicates()
    else:
        print("‚ùå C·∫ßn GPU ƒë·ªÉ ch·∫°y code n√†y.")