import pandas as pd
from transformers import pipeline
import torch
from tqdm import tqdm

# --- C·∫§U H√åNH ---
INPUT_FILE = ["content1", "content2", "content3", "content4"]
MODEL_NAME = "joeddav/xlm-roberta-large-xnli"
BATCH_SIZE = 64  # RTX 3080 10GB/12GB c√¢n t·ªët batch 64 ho·∫∑c 128

# Ki·ªÉm tra GPU
device = 0 if torch.cuda.is_available() else -1
print(f"‚ö° ƒêang ch·∫°y tr√™n: {'GPU (RTX 3080)' if device == 0 else 'CPU'}")

# Kh·ªüi t·∫°o Pipeline
# Model n√†y kho·∫£ng 2.2GB, load v√†o VRAM r·∫•t nh·∫π
classifier = pipeline(
    "zero-shot-classification",
    model=MODEL_NAME,
    device=device
)


def process_files():
    for file_name in INPUT_FILE:
        full_path = "Data/" + file_name + ".csv"
        try:
            print(f"\nüìÇ ƒêang ƒë·ªçc file: {full_path}")
            df = pd.read_csv(full_path)

            # Reset index ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªìng b·ªô
            df_process = df.copy().reset_index(drop=True)

            # T·ª± ƒë·ªông t√¨m c·ªôt n·ªôi dung
            col_name = 'content' if 'content' in df_process.columns else df_process.columns[0]

            # X·ª≠ l√Ω text: Chuy·ªÉn v·ªÅ string, fillna
            # Quan tr·ªçng: C·∫Øt b·ªõt n·∫øu qu√° d√†i ƒë·ªÉ tr√°nh l·ªói model (model n√†y gi·ªõi h·∫°n token)
            all_texts = df_process[col_name].fillna("").astype(str).apply(lambda x: x[:2000]).tolist()

            print(f"üöÄ ƒêang ph√¢n lo·∫°i {len(all_texts)} d√≤ng...")

            # --- C·∫§U H√åNH NH√ÉN (LABELS) ---
            # M·∫πo: D√πng t·ª´ kh√≥a m√¥ t·∫£ h√†nh ƒë·ªông ƒë·ªÉ model d·ªÖ b·∫Øt b√†i Mixue
            candidate_labels = [
                "tin tuy·ªÉn d·ª•ng t√¨m nh√¢n vi√™n",  # Nh√£n m·ª•c ti√™u
                "ng∆∞·ªùi t√¨m vi·ªác l√†m",  # Nh√£n Job Seeker
                "qu·∫£ng c√°o rao v·∫∑t b√°n h√†ng",  # Nh√£n Spam
                "spam r√°c x·ªï s·ªë t√†i ch√≠nh"  # Nh√£n R√°c h·∫≥n
            ]

            # --- CH·∫†Y BATCH ---
            # hypothesis_template c·ª±c quan tr·ªçng cho ti·∫øng Vi·ªát
            results = classifier(
                all_texts,
                candidate_labels,
                hypothesis_template="B√†i vi·∫øt n√†y l√† v·ªÅ {}.",  # Gi√∫p model hi·ªÉu ng·ªØ c·∫£nh
                multi_label=False,
                batch_size=BATCH_SIZE
            )

            # --- X·ª¨ L√ù K·∫æT QU·∫¢ ---
            final_categories = []
            final_scores = []

            for res in results:
                top_label = res['labels'][0]
                score = res['scores'][0]

                # Mapping v·ªÅ code ng·∫Øn g·ªçn
                if top_label == "tin tuy·ªÉn d·ª•ng t√¨m nh√¢n vi√™n":
                    cat = "RECRUITMENT"
                elif top_label == "ng∆∞·ªùi t√¨m vi·ªác l√†m":
                    cat = "JOB_SEEKER"
                else:
                    cat = "SPAM"

                final_categories.append(cat)
                final_scores.append(score)

            # G√°n v√†o DF
            df_process['zs_category'] = final_categories
            df_process['zs_score'] = final_scores

            # --- L∆ØU K·∫æT QU·∫¢ ---
            # L∆∞u file g·ªëc k√®m nh√£n ƒë·ªÉ ki·ªÉm tra
            output_full = "Data/" + file_name + "_labeled_roberta.csv"
            df_process.to_csv(output_full, index=False, encoding='utf-8-sig')

            # Th·ªëng k√™
            n_recruit = len(df_process[df_process['zs_category'] == 'RECRUITMENT'])
            print(f"‚úÖ Ho√†n t·∫•t. T√¨m th·∫•y {n_recruit} b√†i tuy·ªÉn d·ª•ng.")
            print(f"üíæ ƒê√£ l∆∞u: {output_full}")

        except FileNotFoundError:
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {full_path}")
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói: {e}")


if __name__ == "__main__":
    process_files()